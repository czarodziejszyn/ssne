{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKf+7ktOHk69HGq+FnIJT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/czarodziejszyn/ssne/blob/main/projekt6/hate_classificator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQndJb1knoB0",
        "outputId": "eb4d32d5-1111-45e6-edee-73dfccbf8013"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "aahafFxnvQbH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"allegro/herbert-base-cased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "PmoExXoTvRqp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/hate_train.csv\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"sentence\"], df[\"label\"], test_size=0.2)"
      ],
      "metadata": {
        "id": "_YPfvCUfvVJf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class HateDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
        "        self.labels = labels.tolist()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = HateDataset(train_texts, train_labels)\n",
        "val_dataset = HateDataset(val_texts, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "FUcN7-EHvgpP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model.to(DEVICE)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLwNUz6jviUS",
        "outputId": "a4369519-b95f-433e-eb36-fe376a620ae0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHC33M94vjzX",
        "outputId": "9271eb8a-c023-4672-9237-8e8125d9dba3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.0773]\n",
            "Epoch 2: 100%|██████████| 502/502 [02:44<00:00,  3.05it/s, loss=0.0822]\n",
            "Epoch 3: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.168]\n",
            "Epoch 4: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.333]\n",
            "Epoch 5: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.00389]\n",
            "Epoch 6: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.00195]\n",
            "Epoch 7: 100%|██████████| 502/502 [02:44<00:00,  3.05it/s, loss=0.00149]\n",
            "Epoch 8: 100%|██████████| 502/502 [02:44<00:00,  3.06it/s, loss=0.000192]\n",
            "Epoch 9: 100%|██████████| 502/502 [02:44<00:00,  3.05it/s, loss=0.00142]\n",
            "Epoch 10: 100%|██████████| 502/502 [02:44<00:00,  3.05it/s, loss=0.000166]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRHc9TOvmB-",
        "outputId": "959bf472-dd07-46ac-aa03-ea138132cc43"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.93827775012444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      1827\n",
            "           1       0.67      0.62      0.64       182\n",
            "\n",
            "    accuracy                           0.94      2009\n",
            "   macro avg       0.82      0.79      0.80      2009\n",
            "weighted avg       0.94      0.94      0.94      2009\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/hate_test_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for text in sentences:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN)\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "        logits = model(**inputs).logits\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "        predictions.append(str(pred))\n",
        "\n",
        "with open(\"pred.csv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(predictions))"
      ],
      "metadata": {
        "id": "o5v16M9ezUAm"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}