{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZK4KXuEgPNB0a+a6v7h1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/czarodziejszyn/ssne/blob/main/projekt5/recursive_model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY9wWSleQJig",
        "outputId": "804f15a7-d39b-470d-fe55-026d0387b6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "5ZXHpRijSGK9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_path = \"/content/drive/MyDrive/data/train.pkl\"\n",
        "\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "40lSfEtMRnvc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"data length: {len(data)}\")\n",
        "print(f\"element type: {type(data[0])}\")\n",
        "print(f\"song fragment: {data[0][0][:10]}\")\n",
        "print(f\"song class: {data[0][1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQndEeqGSbaD",
        "outputId": "9652fef4-e332-4972-cc00-2d030abcf19e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data length: 2939\n",
            "element type: <class 'tuple'>\n",
            "song fragment: [ -1.  -1.  -1.  -1. 144. 144. 144.  64.  67.   0.]\n",
            "song class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = zip(*data)"
      ],
      "metadata": {
        "id": "K5WD1VDrTBnr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rozkład klas"
      ],
      "metadata": {
        "id": "PV0SYs4jTK9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = Counter(y)\n",
        "print(\"class counts:\")\n",
        "for key, value in class_counts.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3HdBkW9TOAd",
        "outputId": "9a62142e-909c-455e-a6af-3dbe1f0e082c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class counts:\n",
            "0: 1630\n",
            "1: 478\n",
            "2: 154\n",
            "3: 441\n",
            "4: 236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statystyki utworów"
      ],
      "metadata": {
        "id": "ax8A7OcSTkev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(song) for song in X]\n",
        "print(f\"average length: {np.mean(lengths)}\")\n",
        "print(f\"max length: {np.max(lengths)}\")\n",
        "print(f\"min length: {np.min(lengths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyInQseOTgmA",
        "outputId": "5668007d-1093-41c4-c7c6-701f636ffddc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average length: 436.50493365090165\n",
            "max length: 6308\n",
            "min length: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "4mhv-sOLVX4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = zip(*data)\n",
        "y = np.array(y)\n",
        "\n",
        "flattened = np.concatenate(X)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(flattened.reshape(-1, 1))\n",
        "\n",
        "X_scaled = [torch.tensor(scaler.transform(np.array(song).reshape(-1, 1)).flatten(), dtype=torch.float32) for song in X]\n",
        "lengths = torch.tensor([len(song) for song in X_scaled])\n",
        "\n",
        "X_padded = pad_sequence(X_scaled, batch_first=True)\n",
        "\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "X_train, X_val, y_train, y_val, len_train, len_val = train_test_split(\n",
        "    X_padded, y_tensor, lengths, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "-uiAc1ohVXcQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloadery"
      ],
      "metadata": {
        "id": "ij-ijbwrXdYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train, y_train, len_train)\n",
        "val_dataset = TensorDataset(X_val, y_val, len_val)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "FDHM0e4IXfnd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "zmzebdvjYiZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=1, num_classes=5, dropout=0.3):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = x.unsqueeze(-1)\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, cn) = self.lstm(packed)\n",
        "        last_hidden = hn[-1]\n",
        "        out = self.fc(last_hidden)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pyvnR9FpYkAP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trening"
      ],
      "metadata": {
        "id": "ReHZTNs_ZqZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, batch_lengths in data_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            batch_lengths = batch_lengths.to(device)\n",
        "\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    return avg_loss, correct, total\n"
      ],
      "metadata": {
        "id": "QZSryPl_a9O3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_x, batch_y, batch_lengths in train_loader:\n",
        "            batch_x, batch_y, batch_lengths = batch_x.to(device), batch_y.to(device), batch_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_x.size(0)\n",
        "\n",
        "        train_loss /= total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        val_loss, val_correct, val_total = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_correct / val_total:.4f}\")"
      ],
      "metadata": {
        "id": "SgoKuUR6Zrik"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GypQrRhbFia",
        "outputId": "993be69b-1c7c-4422-a59b-bcd131c1eb71"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.3528 | Train Acc: 0.5432 | Val Loss: 1.2681 | Val Acc: 0.5544\n",
            "Epoch 02 | Train Loss: 1.2501 | Train Acc: 0.5619 | Val Loss: 1.2298 | Val Acc: 0.5510\n",
            "Epoch 03 | Train Loss: 1.2215 | Train Acc: 0.5751 | Val Loss: 1.2353 | Val Acc: 0.5697\n",
            "Epoch 04 | Train Loss: 1.2307 | Train Acc: 0.5700 | Val Loss: 1.2666 | Val Acc: 0.5102\n",
            "Epoch 05 | Train Loss: 1.2301 | Train Acc: 0.5525 | Val Loss: 1.2309 | Val Acc: 0.5544\n",
            "Epoch 06 | Train Loss: 1.2364 | Train Acc: 0.5547 | Val Loss: 1.1994 | Val Acc: 0.5544\n",
            "Epoch 07 | Train Loss: 1.1973 | Train Acc: 0.5547 | Val Loss: 1.2038 | Val Acc: 0.5544\n",
            "Epoch 08 | Train Loss: 1.2316 | Train Acc: 0.5547 | Val Loss: 1.2526 | Val Acc: 0.5544\n",
            "Epoch 09 | Train Loss: 1.2522 | Train Acc: 0.5547 | Val Loss: 1.2409 | Val Acc: 0.5544\n",
            "Epoch 10 | Train Loss: 1.2220 | Train Acc: 0.5547 | Val Loss: 1.2018 | Val Acc: 0.5544\n"
          ]
        }
      ]
    },
    {
      "source": [
        "class BidirectionalRNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=1, num_classes=5, dropout=0.3):\n",
        "        super(BidirectionalRNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = x.unsqueeze(-1)\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, cn) = self.lstm(packed)\n",
        "        last_hidden = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
        "        out = self.fc(last_hidden)\n",
        "        return out\n",
        "\n",
        "class_counts = Counter(y)\n",
        "total_samples = sum(class_counts.values())\n",
        "class_weights = torch.tensor([total_samples / class_counts[i] for i in range(len(class_counts))], dtype=torch.float32)\n",
        "if torch.cuda.is_available():\n",
        "    class_weights = class_weights.to('cuda')\n",
        "\n",
        "def train_model_weighted(model, train_loader, val_loader, class_weights, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_x, batch_y, batch_lengths in train_loader:\n",
        "            batch_x, batch_y, batch_lengths = batch_x.to(device), batch_y.to(device), batch_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_x.size(0)\n",
        "\n",
        "        train_loss /= total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        val_loss, val_correct, val_total = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_correct / val_total:.4f}\")\n",
        "\n",
        "model_improved = BidirectionalRNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.5\n",
        ")\n",
        "\n",
        "train_model_weighted(model_improved, train_loader, val_loader, class_weights, num_epochs=100, lr=4e-4)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHli3QKGctJR",
        "outputId": "120ab916-f1e4-4525-d290-a89b110a242a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.6051 | Train Acc: 0.0940 | Val Loss: 1.5901 | Val Acc: 0.2228\n",
            "Epoch 02 | Train Loss: 1.5374 | Train Acc: 0.2658 | Val Loss: 1.5378 | Val Acc: 0.2908\n",
            "Epoch 03 | Train Loss: 1.4881 | Train Acc: 0.3696 | Val Loss: 1.5096 | Val Acc: 0.3129\n",
            "Epoch 04 | Train Loss: 1.4518 | Train Acc: 0.4151 | Val Loss: 1.4825 | Val Acc: 0.3214\n",
            "Epoch 05 | Train Loss: 1.4483 | Train Acc: 0.3645 | Val Loss: 1.4894 | Val Acc: 0.4303\n",
            "Epoch 06 | Train Loss: 1.4136 | Train Acc: 0.3854 | Val Loss: 1.4691 | Val Acc: 0.3316\n",
            "Epoch 07 | Train Loss: 1.4014 | Train Acc: 0.3892 | Val Loss: 1.4895 | Val Acc: 0.3588\n",
            "Epoch 08 | Train Loss: 1.4585 | Train Acc: 0.3769 | Val Loss: 1.4513 | Val Acc: 0.3146\n",
            "Epoch 09 | Train Loss: 1.4113 | Train Acc: 0.3756 | Val Loss: 1.4392 | Val Acc: 0.4405\n",
            "Epoch 10 | Train Loss: 1.3993 | Train Acc: 0.4607 | Val Loss: 1.3920 | Val Acc: 0.4728\n",
            "Epoch 11 | Train Loss: 1.3724 | Train Acc: 0.4772 | Val Loss: 1.3884 | Val Acc: 0.5068\n",
            "Epoch 12 | Train Loss: 1.3664 | Train Acc: 0.4866 | Val Loss: 1.4918 | Val Acc: 0.3963\n",
            "Epoch 13 | Train Loss: 1.4290 | Train Acc: 0.4079 | Val Loss: 1.4862 | Val Acc: 0.3588\n",
            "Epoch 14 | Train Loss: 1.4065 | Train Acc: 0.3709 | Val Loss: 1.4807 | Val Acc: 0.4490\n",
            "Epoch 15 | Train Loss: 1.4186 | Train Acc: 0.4143 | Val Loss: 1.4893 | Val Acc: 0.3724\n",
            "Epoch 16 | Train Loss: 1.3972 | Train Acc: 0.4343 | Val Loss: 1.4595 | Val Acc: 0.3605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/rnn_classifier.pth\"\n",
        "torch.save(model_improved.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "BGRXMkech0N1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/rnn_classifier.pth\"\n",
        "loaded_model = BidirectionalRNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "loaded_model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAh9rDp5iF4_",
        "outputId": "163f2bba-bca5-42ea-8b79-1a5537949366"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "source": [
        "train_model_weighted(loaded_model, train_loader, val_loader, class_weights, num_epochs=30, lr=5e-4)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGwhRIFDiOGR",
        "outputId": "684b010b-7265-4f3c-dcb9-0fae03623c57"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.4207 | Train Acc: 0.7912 | Val Loss: 1.6686 | Val Acc: 0.6054\n",
            "Epoch 02 | Train Loss: 0.3381 | Train Acc: 0.8358 | Val Loss: 1.8129 | Val Acc: 0.5952\n",
            "Epoch 03 | Train Loss: 0.2877 | Train Acc: 0.8481 | Val Loss: 1.9280 | Val Acc: 0.5867\n",
            "Epoch 04 | Train Loss: 0.2839 | Train Acc: 0.8537 | Val Loss: 1.9376 | Val Acc: 0.6088\n",
            "Epoch 05 | Train Loss: 0.2657 | Train Acc: 0.8647 | Val Loss: 1.9524 | Val Acc: 0.6327\n",
            "Epoch 06 | Train Loss: 0.2545 | Train Acc: 0.8732 | Val Loss: 1.9085 | Val Acc: 0.6224\n",
            "Epoch 07 | Train Loss: 0.2072 | Train Acc: 0.8894 | Val Loss: 2.0402 | Val Acc: 0.6054\n",
            "Epoch 08 | Train Loss: 0.2108 | Train Acc: 0.8898 | Val Loss: 2.0417 | Val Acc: 0.5952\n",
            "Epoch 09 | Train Loss: 0.1625 | Train Acc: 0.9183 | Val Loss: 2.0739 | Val Acc: 0.6344\n",
            "Epoch 10 | Train Loss: 0.1464 | Train Acc: 0.9307 | Val Loss: 2.1760 | Val Acc: 0.6054\n",
            "Epoch 11 | Train Loss: 0.1568 | Train Acc: 0.9200 | Val Loss: 2.2034 | Val Acc: 0.6207\n",
            "Epoch 12 | Train Loss: 0.1065 | Train Acc: 0.9536 | Val Loss: 2.2570 | Val Acc: 0.6446\n",
            "Epoch 13 | Train Loss: 0.1464 | Train Acc: 0.9205 | Val Loss: 2.2174 | Val Acc: 0.6378\n",
            "Epoch 14 | Train Loss: 0.1195 | Train Acc: 0.9451 | Val Loss: 2.2805 | Val Acc: 0.6463\n",
            "Epoch 15 | Train Loss: 0.1025 | Train Acc: 0.9502 | Val Loss: 2.2332 | Val Acc: 0.6310\n",
            "Epoch 16 | Train Loss: 0.1189 | Train Acc: 0.9460 | Val Loss: 2.3018 | Val Acc: 0.6310\n",
            "Epoch 17 | Train Loss: 0.0878 | Train Acc: 0.9621 | Val Loss: 2.3209 | Val Acc: 0.6412\n",
            "Epoch 18 | Train Loss: 0.0536 | Train Acc: 0.9834 | Val Loss: 2.4464 | Val Acc: 0.6310\n",
            "Epoch 19 | Train Loss: 0.0403 | Train Acc: 0.9898 | Val Loss: 2.4820 | Val Acc: 0.6327\n",
            "Epoch 20 | Train Loss: 0.0307 | Train Acc: 0.9945 | Val Loss: 2.5249 | Val Acc: 0.6514\n",
            "Epoch 21 | Train Loss: 0.0230 | Train Acc: 0.9979 | Val Loss: 2.6181 | Val Acc: 0.6548\n",
            "Epoch 22 | Train Loss: 0.0188 | Train Acc: 0.9996 | Val Loss: 2.6475 | Val Acc: 0.6565\n",
            "Epoch 23 | Train Loss: 0.0166 | Train Acc: 0.9996 | Val Loss: 2.7209 | Val Acc: 0.6616\n",
            "Epoch 24 | Train Loss: 0.0136 | Train Acc: 1.0000 | Val Loss: 2.7599 | Val Acc: 0.6565\n",
            "Epoch 25 | Train Loss: 0.0119 | Train Acc: 1.0000 | Val Loss: 2.7451 | Val Acc: 0.6582\n",
            "Epoch 26 | Train Loss: 0.0105 | Train Acc: 1.0000 | Val Loss: 2.7730 | Val Acc: 0.6514\n",
            "Epoch 27 | Train Loss: 0.0095 | Train Acc: 1.0000 | Val Loss: 2.7820 | Val Acc: 0.6497\n",
            "Epoch 28 | Train Loss: 0.0103 | Train Acc: 1.0000 | Val Loss: 2.8693 | Val Acc: 0.6531\n",
            "Epoch 29 | Train Loss: 0.0088 | Train Acc: 1.0000 | Val Loss: 2.9005 | Val Acc: 0.6429\n",
            "Epoch 30 | Train Loss: 0.0076 | Train Acc: 1.0000 | Val Loss: 2.9168 | Val Acc: 0.6616\n"
          ]
        }
      ]
    }
  ]
}