{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfwSc04Phtnir/F66Xfob0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/czarodziejszyn/ssne/blob/main/projekt/recursive_model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY9wWSleQJig",
        "outputId": "804f15a7-d39b-470d-fe55-026d0387b6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "5ZXHpRijSGK9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_path = \"/content/drive/MyDrive/data/train.pkl\"\n",
        "\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "40lSfEtMRnvc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"data length: {len(data)}\")\n",
        "print(f\"element type: {type(data[0])}\")\n",
        "print(f\"song fragment: {data[0][0][:10]}\")\n",
        "print(f\"song class: {data[0][1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQndEeqGSbaD",
        "outputId": "9652fef4-e332-4972-cc00-2d030abcf19e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data length: 2939\n",
            "element type: <class 'tuple'>\n",
            "song fragment: [ -1.  -1.  -1.  -1. 144. 144. 144.  64.  67.   0.]\n",
            "song class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = zip(*data)"
      ],
      "metadata": {
        "id": "K5WD1VDrTBnr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rozkład klas"
      ],
      "metadata": {
        "id": "PV0SYs4jTK9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = Counter(y)\n",
        "print(\"class counts:\")\n",
        "for key, value in class_counts.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3HdBkW9TOAd",
        "outputId": "9a62142e-909c-455e-a6af-3dbe1f0e082c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class counts:\n",
            "0: 1630\n",
            "1: 478\n",
            "2: 154\n",
            "3: 441\n",
            "4: 236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statystyki utworów"
      ],
      "metadata": {
        "id": "ax8A7OcSTkev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(song) for song in X]\n",
        "print(f\"average length: {np.mean(lengths)}\")\n",
        "print(f\"max length: {np.max(lengths)}\")\n",
        "print(f\"min length: {np.min(lengths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyInQseOTgmA",
        "outputId": "5668007d-1093-41c4-c7c6-701f636ffddc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average length: 436.50493365090165\n",
            "max length: 6308\n",
            "min length: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "4mhv-sOLVX4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = zip(*data)\n",
        "y = np.array(y)\n",
        "\n",
        "flattened = np.concatenate(X)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(flattened.reshape(-1, 1))\n",
        "\n",
        "X_scaled = [torch.tensor(scaler.transform(np.array(song).reshape(-1, 1)).flatten(), dtype=torch.float32) for song in X]\n",
        "lengths = torch.tensor([len(song) for song in X_scaled])\n",
        "\n",
        "X_padded = pad_sequence(X_scaled, batch_first=True)\n",
        "\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "X_train, X_val, y_train, y_val, len_train, len_val = train_test_split(\n",
        "    X_padded, y_tensor, lengths, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "-uiAc1ohVXcQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloadery"
      ],
      "metadata": {
        "id": "ij-ijbwrXdYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train, y_train, len_train)\n",
        "val_dataset = TensorDataset(X_val, y_val, len_val)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "FDHM0e4IXfnd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "zmzebdvjYiZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=1, num_classes=5, dropout=0.3):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = x.unsqueeze(-1)\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, cn) = self.lstm(packed)\n",
        "        last_hidden = hn[-1]\n",
        "        out = self.fc(last_hidden)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pyvnR9FpYkAP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trening"
      ],
      "metadata": {
        "id": "ReHZTNs_ZqZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y, batch_lengths in data_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            batch_lengths = batch_lengths.to(device)\n",
        "\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    return avg_loss, correct, total\n"
      ],
      "metadata": {
        "id": "QZSryPl_a9O3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_x, batch_y, batch_lengths in train_loader:\n",
        "            batch_x, batch_y, batch_lengths = batch_x.to(device), batch_y.to(device), batch_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_x.size(0)\n",
        "\n",
        "        train_loss /= total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        val_loss, val_correct, val_total = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_correct / val_total:.4f}\")"
      ],
      "metadata": {
        "id": "SgoKuUR6Zrik"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GypQrRhbFia",
        "outputId": "993be69b-1c7c-4422-a59b-bcd131c1eb71"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.3528 | Train Acc: 0.5432 | Val Loss: 1.2681 | Val Acc: 0.5544\n",
            "Epoch 02 | Train Loss: 1.2501 | Train Acc: 0.5619 | Val Loss: 1.2298 | Val Acc: 0.5510\n",
            "Epoch 03 | Train Loss: 1.2215 | Train Acc: 0.5751 | Val Loss: 1.2353 | Val Acc: 0.5697\n",
            "Epoch 04 | Train Loss: 1.2307 | Train Acc: 0.5700 | Val Loss: 1.2666 | Val Acc: 0.5102\n",
            "Epoch 05 | Train Loss: 1.2301 | Train Acc: 0.5525 | Val Loss: 1.2309 | Val Acc: 0.5544\n",
            "Epoch 06 | Train Loss: 1.2364 | Train Acc: 0.5547 | Val Loss: 1.1994 | Val Acc: 0.5544\n",
            "Epoch 07 | Train Loss: 1.1973 | Train Acc: 0.5547 | Val Loss: 1.2038 | Val Acc: 0.5544\n",
            "Epoch 08 | Train Loss: 1.2316 | Train Acc: 0.5547 | Val Loss: 1.2526 | Val Acc: 0.5544\n",
            "Epoch 09 | Train Loss: 1.2522 | Train Acc: 0.5547 | Val Loss: 1.2409 | Val Acc: 0.5544\n",
            "Epoch 10 | Train Loss: 1.2220 | Train Acc: 0.5547 | Val Loss: 1.2018 | Val Acc: 0.5544\n"
          ]
        }
      ]
    },
    {
      "source": [
        "class BidirectionalRNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, num_layers=1, num_classes=5, dropout=0.3):\n",
        "        super(BidirectionalRNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True # Dwukierunkowe\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = x.unsqueeze(-1)\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, cn) = self.lstm(packed)\n",
        "        last_hidden = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
        "        out = self.fc(last_hidden)\n",
        "        return out\n",
        "\n",
        "class_counts = Counter(y)\n",
        "total_samples = sum(class_counts.values())\n",
        "class_weights = torch.tensor([total_samples / class_counts[i] for i in range(len(class_counts))], dtype=torch.float32)\n",
        "if torch.cuda.is_available():\n",
        "    class_weights = class_weights.to('cuda')\n",
        "\n",
        "def train_model_weighted(model, train_loader, val_loader, class_weights, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "        for batch_x, batch_y, batch_lengths in train_loader:\n",
        "            batch_x, batch_y, batch_lengths = batch_x.to(device), batch_y.to(device), batch_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x, batch_lengths)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_x.size(0)\n",
        "\n",
        "        train_loss /= total\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        val_loss, val_correct, val_total = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_correct / val_total:.4f}\")\n",
        "\n",
        "model_improved = BidirectionalRNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "train_model_weighted(model_improved, train_loader, val_loader, class_weights, num_epochs=50, lr=5e-4)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHli3QKGctJR",
        "outputId": "6b138e96-068f-4746-ba32-58a51a86290d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.5915 | Train Acc: 0.3050 | Val Loss: 1.5619 | Val Acc: 0.2568\n",
            "Epoch 02 | Train Loss: 1.5269 | Train Acc: 0.3228 | Val Loss: 1.5251 | Val Acc: 0.3673\n",
            "Epoch 03 | Train Loss: 1.5110 | Train Acc: 0.3028 | Val Loss: 1.5167 | Val Acc: 0.2551\n",
            "Epoch 04 | Train Loss: 1.4747 | Train Acc: 0.3071 | Val Loss: 1.5050 | Val Acc: 0.3146\n",
            "Epoch 05 | Train Loss: 1.4659 | Train Acc: 0.3471 | Val Loss: 1.5000 | Val Acc: 0.3180\n",
            "Epoch 06 | Train Loss: 1.4219 | Train Acc: 0.4024 | Val Loss: 1.4734 | Val Acc: 0.3963\n",
            "Epoch 07 | Train Loss: 1.4198 | Train Acc: 0.4058 | Val Loss: 1.4810 | Val Acc: 0.4456\n",
            "Epoch 08 | Train Loss: 1.4023 | Train Acc: 0.4079 | Val Loss: 1.4798 | Val Acc: 0.3231\n",
            "Epoch 09 | Train Loss: 1.3984 | Train Acc: 0.4224 | Val Loss: 1.4614 | Val Acc: 0.3486\n",
            "Epoch 10 | Train Loss: 1.3786 | Train Acc: 0.4032 | Val Loss: 1.4575 | Val Acc: 0.4048\n",
            "Epoch 11 | Train Loss: 1.4233 | Train Acc: 0.4198 | Val Loss: 1.4892 | Val Acc: 0.2874\n",
            "Epoch 12 | Train Loss: 1.3793 | Train Acc: 0.4041 | Val Loss: 1.4552 | Val Acc: 0.4133\n",
            "Epoch 13 | Train Loss: 1.3409 | Train Acc: 0.4615 | Val Loss: 1.4576 | Val Acc: 0.4065\n",
            "Epoch 14 | Train Loss: 1.3446 | Train Acc: 0.4436 | Val Loss: 1.4265 | Val Acc: 0.4507\n",
            "Epoch 15 | Train Loss: 1.4173 | Train Acc: 0.4275 | Val Loss: 1.4814 | Val Acc: 0.3707\n",
            "Epoch 16 | Train Loss: 1.4686 | Train Acc: 0.3216 | Val Loss: 1.4836 | Val Acc: 0.2908\n",
            "Epoch 17 | Train Loss: 1.4183 | Train Acc: 0.3296 | Val Loss: 1.4989 | Val Acc: 0.3112\n",
            "Epoch 18 | Train Loss: 1.4119 | Train Acc: 0.3645 | Val Loss: 1.4840 | Val Acc: 0.3435\n",
            "Epoch 19 | Train Loss: 1.4687 | Train Acc: 0.3445 | Val Loss: 1.4809 | Val Acc: 0.3639\n",
            "Epoch 20 | Train Loss: 1.3938 | Train Acc: 0.3590 | Val Loss: 1.4726 | Val Acc: 0.3486\n",
            "Epoch 21 | Train Loss: 1.3675 | Train Acc: 0.3849 | Val Loss: 1.4817 | Val Acc: 0.4150\n",
            "Epoch 22 | Train Loss: 1.3600 | Train Acc: 0.4228 | Val Loss: 1.4789 | Val Acc: 0.4116\n",
            "Epoch 23 | Train Loss: 1.3333 | Train Acc: 0.4373 | Val Loss: 1.4774 | Val Acc: 0.3861\n",
            "Epoch 24 | Train Loss: 1.3422 | Train Acc: 0.4224 | Val Loss: 1.4577 | Val Acc: 0.4065\n",
            "Epoch 25 | Train Loss: 1.3181 | Train Acc: 0.4360 | Val Loss: 1.4253 | Val Acc: 0.4354\n",
            "Epoch 26 | Train Loss: 1.2897 | Train Acc: 0.4675 | Val Loss: 1.4476 | Val Acc: 0.4269\n",
            "Epoch 27 | Train Loss: 1.2715 | Train Acc: 0.4747 | Val Loss: 1.4642 | Val Acc: 0.3588\n",
            "Epoch 28 | Train Loss: 1.3008 | Train Acc: 0.4513 | Val Loss: 1.4268 | Val Acc: 0.4354\n",
            "Epoch 29 | Train Loss: 1.2582 | Train Acc: 0.4581 | Val Loss: 1.4061 | Val Acc: 0.4133\n",
            "Epoch 30 | Train Loss: 1.2373 | Train Acc: 0.4926 | Val Loss: 1.4006 | Val Acc: 0.4422\n",
            "Epoch 31 | Train Loss: 1.2220 | Train Acc: 0.4840 | Val Loss: 1.3912 | Val Acc: 0.4864\n",
            "Epoch 32 | Train Loss: 1.2510 | Train Acc: 0.4828 | Val Loss: 1.4285 | Val Acc: 0.4983\n",
            "Epoch 33 | Train Loss: 1.2028 | Train Acc: 0.5164 | Val Loss: 1.3829 | Val Acc: 0.4847\n",
            "Epoch 34 | Train Loss: 1.2438 | Train Acc: 0.4441 | Val Loss: 1.4125 | Val Acc: 0.3776\n",
            "Epoch 35 | Train Loss: 1.1802 | Train Acc: 0.5049 | Val Loss: 1.4379 | Val Acc: 0.4167\n",
            "Epoch 36 | Train Loss: 1.2405 | Train Acc: 0.4619 | Val Loss: 1.5378 | Val Acc: 0.3418\n",
            "Epoch 37 | Train Loss: 1.2989 | Train Acc: 0.4556 | Val Loss: 1.3772 | Val Acc: 0.4116\n",
            "Epoch 38 | Train Loss: 1.2416 | Train Acc: 0.5028 | Val Loss: 1.3888 | Val Acc: 0.4745\n",
            "Epoch 39 | Train Loss: 1.1938 | Train Acc: 0.5177 | Val Loss: 1.4705 | Val Acc: 0.3486\n",
            "Epoch 40 | Train Loss: 1.1798 | Train Acc: 0.4883 | Val Loss: 1.3868 | Val Acc: 0.4320\n",
            "Epoch 41 | Train Loss: 1.1424 | Train Acc: 0.5206 | Val Loss: 1.4095 | Val Acc: 0.4337\n",
            "Epoch 42 | Train Loss: 1.1245 | Train Acc: 0.5521 | Val Loss: 1.4245 | Val Acc: 0.5221\n",
            "Epoch 43 | Train Loss: 1.1053 | Train Acc: 0.5325 | Val Loss: 1.4075 | Val Acc: 0.4830\n",
            "Epoch 44 | Train Loss: 1.0665 | Train Acc: 0.5576 | Val Loss: 1.4232 | Val Acc: 0.3963\n",
            "Epoch 45 | Train Loss: 1.0976 | Train Acc: 0.5547 | Val Loss: 1.4044 | Val Acc: 0.5136\n",
            "Epoch 46 | Train Loss: 1.0404 | Train Acc: 0.5674 | Val Loss: 1.3375 | Val Acc: 0.4541\n",
            "Epoch 47 | Train Loss: 0.9881 | Train Acc: 0.5772 | Val Loss: 1.3727 | Val Acc: 0.4541\n",
            "Epoch 48 | Train Loss: 0.9468 | Train Acc: 0.6151 | Val Loss: 1.3396 | Val Acc: 0.5289\n",
            "Epoch 49 | Train Loss: 0.9175 | Train Acc: 0.5925 | Val Loss: 1.3456 | Val Acc: 0.5697\n",
            "Epoch 50 | Train Loss: 0.8765 | Train Acc: 0.6048 | Val Loss: 1.3506 | Val Acc: 0.5357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/rnn_classifier.pth\"\n",
        "torch.save(model_improved.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "BGRXMkech0N1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/rnn_classifier.pth\"\n",
        "loaded_model = BidirectionalRNNClassifier(\n",
        "    input_size=1,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "loaded_model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAh9rDp5iF4_",
        "outputId": "163f2bba-bca5-42ea-8b79-1a5537949366"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "source": [
        "train_model_weighted(loaded_model, train_loader, val_loader, class_weights, num_epochs=20, lr=5e-4)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGwhRIFDiOGR",
        "outputId": "dfeaac83-6baa-41b9-bb39-7222ab5389e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.8827 | Train Acc: 0.6146 | Val Loss: 1.3836 | Val Acc: 0.4966\n",
            "Epoch 02 | Train Loss: 0.8157 | Train Acc: 0.6380 | Val Loss: 1.4244 | Val Acc: 0.5119\n",
            "Epoch 03 | Train Loss: 0.7482 | Train Acc: 0.6678 | Val Loss: 1.3995 | Val Acc: 0.4932\n",
            "Epoch 04 | Train Loss: 0.7391 | Train Acc: 0.6635 | Val Loss: 1.4094 | Val Acc: 0.5459\n",
            "Epoch 05 | Train Loss: 0.6846 | Train Acc: 0.6874 | Val Loss: 1.4285 | Val Acc: 0.5187\n",
            "Epoch 06 | Train Loss: 0.6608 | Train Acc: 0.6997 | Val Loss: 1.5349 | Val Acc: 0.4983\n",
            "Epoch 07 | Train Loss: 0.6213 | Train Acc: 0.7167 | Val Loss: 1.4433 | Val Acc: 0.5663\n",
            "Epoch 08 | Train Loss: 0.6169 | Train Acc: 0.7291 | Val Loss: 1.4767 | Val Acc: 0.5561\n"
          ]
        }
      ]
    }
  ]
}